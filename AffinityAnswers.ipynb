{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imagine there is a file full of Twitter tweets by various users and you are provided a set of words that indicates racial slurs. Write a program that can indicate the degree of profanity for each sentence in the file. Write in any programming language (preferably in Python) - make any assumptions, but remember to state them. Please place the code in GitHub with proper documentation and share. Please respond in 2-3 days."
      ],
      "metadata": {
        "id": "590vYQWB8qjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ASSUMPTIONS:**\n",
        "\n",
        "1. The twitter tweets are in a csv file named like \"tweets.csv\".\n",
        "\n",
        "2. Set of words that indicates racial slurs are within a text file like \"racial_slurs.txt\".\n",
        "\n",
        "3. Each word in text file is seperated by space \n",
        "\n",
        "4. The tweets contains lot of hastags \"#\", metiones (\"@\"), links (\"https:.....\") and other messy words.\n",
        "\n",
        "5. It contains total of 10,00,000 tweets\n",
        "\n",
        "6. The columns are Tweet_id, Author_name, tweet.\n",
        "\n",
        "7. The shape of dataset is (1000000,3)\n",
        "\n",
        "8. All tweets are in only english language\n",
        "\n",
        "9. The dataset contains some Nan values but in a small amount like (1000).\n",
        "\n",
        "10. The daset also contains emojis\n",
        "\n"
      ],
      "metadata": {
        "id": "tBvIdUvj84wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re"
      ],
      "metadata": {
        "id": "L_MA8cU182Aw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the dataset\n",
        "data=pd.read_csv(\"tweets.csv\")\n",
        "data.dropna(inplace=True)  #All rows with nan values are dropped, in our case it is 1000"
      ],
      "metadata": {
        "id": "kT-IS3fc8t1b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file=open(\"racial_slurs.txt\",\"r\")\n",
        "words=[] \n",
        "for i in file:\n",
        "  for j in i.split():\n",
        "    words.append(j)"
      ],
      "metadata": {
        "id": "60-xLkAT_yju"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The slur words present in the text file are now stored in a list named \"words\""
      ],
      "metadata": {
        "id": "K_VhewpsAo4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's clean the text**"
      ],
      "metadata": {
        "id": "DU2TbidFA-7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Text cleaning\n",
        "from cleantext import clean\n",
        "lemma=WordNetLemmatizer()\n",
        "n=[]\n",
        "def text_cleaning(i):\n",
        "  cleaned=clean(i,no_emoji=True)   #Removing emojis present in the tweet\n",
        "  cleaned=cleaned.lower()          #Lower casing the all words\n",
        "  url = re.compile(r'https?://\\S+|www\\.\\S+')  # Regular expression pattern to match a URL\n",
        "  cleaned_text = re.sub(url, '', cleaned)     # Replace all URLs in the text with an empty string\n",
        "  s=re.sub(\"[^a-zA-z0-9]\",\" \",cleaned_text)\n",
        "  s=s.split()\n",
        "  k=[]\n",
        "  for j in s:\n",
        "      if j not in stopwords.words(\"english\"):\n",
        "          k.append(lemma.lemmatize(j))\n",
        "    n.append(\" \".join(k))"
      ],
      "metadata": {
        "id": "_cGWefEoChP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the text cleaning is done. The text will be free from emojis, numbers, hashtags, mentiones, links. Let's find the degree of profanity in the upcomming steps."
      ],
      "metadata": {
        "id": "SQTrf33ZEV4w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['cleaned_text']=n"
      ],
      "metadata": {
        "id": "KnP1bctVEzuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lets create a function to calculate degree of profanity\n",
        "\n",
        "def count_profanity(text):\n",
        "  k=0\n",
        "  t=text.split()\n",
        "  for i in t:\n",
        "    if i in words:\n",
        "      k+=1\n",
        "  return k"
      ],
      "metadata": {
        "id": "ZjzmZv-cE3nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"degree_of_profanity\"]=data[\"cleaned_text\"].apply(lambda x:count_profanity(x))"
      ],
      "metadata": {
        "id": "R1lzpp_LGFYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Now the dataframe will look like\n",
        "\n",
        "----------------------------------------------------------------------------------------------\n",
        "Tweet_id | Author_name |          tweet                   |    Cleaned_text   | degree_of_prfanity \n",
        "----------------------------------------------------------------------------------------------\n",
        "\n",
        "T12232     Black_pearl     @world, This is not updated        world updated         0\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "r9hAPZiFGRYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df=data.to_csv(\"final_df.csv\")"
      ],
      "metadata": {
        "id": "OTyfG4IVHTl_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}